import re
import json
from underthesea import word_tokenize, sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import streamlit as st

# 1. Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n

def preprocess_text(text):
    """
    Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, t√°ch t·ª´.
    """
    # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
    text = text.lower()
    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† s·ªë
    text = re.sub(r'[^\w\s]', '', text)  # Lo·∫°i b·ªè d·∫•u c√¢u
    text = re.sub(r'\d+', '', text)      # Lo·∫°i b·ªè s·ªë
    # T√°ch t·ª´
    tokens = word_tokenize(text)
    # Gh√©p l·∫°i th√†nh c√¢u
    return " ".join(tokens)

# 2. Load d·ªØ li·ªáu FAQ v√† ti·ªÅn x·ª≠ l√Ω c√¢u h·ªèi
with open('./faq_data (2).json', 'r', encoding='utf-8') as file:
    faq_data = json.load(file)

questions = [item["question"] for item in faq_data]
answers = [item["answer"] for item in faq_data]

# Ti·ªÅn x·ª≠ l√Ω c√°c c√¢u h·ªèi trong FAQ
preprocessed_questions = [preprocess_text(q) for q in questions]

# 3. T·∫°o TF-IDF vectorizer t·ª´ c√¢u h·ªèi ƒë√£ ti·ªÅn x·ª≠ l√Ω
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(preprocessed_questions)

# 4. H√†m t√≥m t·∫Øt vƒÉn b·∫£n
def summarize_text(text, max_sentences=3):
    """
    T√≥m t·∫Øt c√¢u tr·∫£ l·ªùi: ch·ªâ l·∫•y m·ªôt s·ªë c√¢u ƒë·∫ßu ti√™n n·∫øu c√¢u tr·∫£ l·ªùi qu√° d√†i.
    """
    sentences = sent_tokenize(text)  # Chia ƒëo·∫°n vƒÉn th√†nh c√°c c√¢u b·∫±ng Underthesea
    if len(sentences) > max_sentences:
        return " ".join(sentences[:max_sentences]) + "..."
    return text

# 5. H√†m t√¨m ki·∫øm c√¢u h·ªèi t∆∞∆°ng t·ª±
def search_faq(user_input, threshold=0.5):
    """
    T√¨m c√¢u h·ªèi trong FAQ t∆∞∆°ng t·ª± nh·∫•t v·ªõi ƒë·∫ßu v√†o c·ªßa ng∆∞·ªùi d√πng.
    """
    # Ti·ªÅn x·ª≠ l√Ω c√¢u h·ªèi ƒë·∫ßu v√†o
    preprocessed_input = preprocess_text(user_input)
    # Bi·∫øn ƒë·ªïi th√†nh vector TF-IDF
    user_tfidf = vectorizer.transform([preprocessed_input])
    similarities = cosine_similarity(user_tfidf, tfidf_matrix).flatten()
    best_index = similarities.argmax()

    if similarities[best_index] >= threshold:
        return questions[best_index], answers[best_index], similarities[best_index]
    return None, "Xin l·ªói, kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ph√π h·ª£p.", 0.0

# 6. Giao di·ªán Streamlit
st.title("H·ªèi ƒë√°p ng√†nh h·ªçc - HUMG")
st.markdown("""
### Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ƒë·ªÉ t√¨m hi·ªÉu th√™m v·ªÅ c√°c ng√†nh h·ªçc
H·ªá th·ªëng cung c·∫•p c√°c th√¥ng tin v·ªÅ:
- ‚úÖ **Th√¥ng tin t·ªïng quan**
- üíº **C∆° h·ªôi vi·ªác l√†m**
- üìö **ƒêi·ªÅu ki·ªán h·ªçc t·∫≠p v√† ch√≠nh s√°ch h·ªó tr·ª£ sinh vi√™n**
- üïí **Th·ªùi gian ƒë√†o t·∫°o v√† m·ª©c h·ªçc ph√≠**
- üéì **C∆° h·ªôi h·ªçc b·ªïng v√† m·ª©c l∆∞∆°ng sau khi ra tr∆∞·ªùng**
- üìû **Th√¥ng tin li√™n h·ªá c·ªßa c√°c ng√†nh h·ªçc**
""", unsafe_allow_html=True)

# Nh·∫≠n c√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
user_question = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:")

if user_question:
    matched_question, answer, similarity = search_faq(user_question)
    if similarity > 0:
        # T√≥m t·∫Øt c√¢u tr·∫£ l·ªùi n·∫øu qu√° d√†i
        summarized_answer = summarize_text(answer, max_sentences=3)
        
        st.write(f"**C√¢u h·ªèi g·∫ßn nh·∫•t:** {matched_question}")
        st.write(f"**C√¢u tr·∫£ l·ªùi t√≥m t·∫Øt:** {summarized_answer}")
        st.write(f"**ƒê·ªô t∆∞∆°ng ƒë·ªìng:** {similarity:.2f}")
        with st.expander("Xem ƒë·∫ßy ƒë·ªß c√¢u tr·∫£ l·ªùi"):
            st.write(answer)
    else:
        st.write(answer)
import pandas as pd

